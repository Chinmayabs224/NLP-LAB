{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98d19904-3f97-4605-925b-2b90b6d04855",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #49A, #0FB); color: white; padding: 20px;\">program 3</h1>\n",
    "\n",
    "<ol start=\"3\">\n",
    "    <li>\n",
    "        <ol type=\"a\">\n",
    "            <li>Write a program to implement N-Gram Model from the given text, excluding the stop words:\n",
    "               <blockquote>\n",
    "                   \"Artificial intelligence has made significant advancements, but it still struggles to understand human emotions and context, limiting its ability to interact naturally.\"\n",
    "               </blockquote>\n",
    "            </li>\n",
    "            <li>Write a Python program using the NLTK library to process a sentence and generate Part-of-Speech (POS) tags with the averaged perceptron tagger.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00eca1a-8d21-44b5-b0bc-ce40c09f5896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('artificial', 'intelligence'), ('intelligence', 'made'), ('made', 'significant'), ('significant', 'advancements'), ('advancements', 'still'), ('still', 'struggles'), ('struggles', 'understand'), ('understand', 'human'), ('human', 'emotions'), ('emotions', 'context'), ('context', 'limiting'), ('limiting', 'ability'), ('ability', 'interact'), ('interact', 'naturally')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"Artificial intelligence has made significant advancements, but it still struggles to understand human emotions and context, limiting its ability to interact naturally.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in word_tokenize(text.lower()) if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Generate Bigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(\"Bigrams:\", bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456b4909-6e69-4a8b-ab20-ac4a4d849f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [WinError 10053] An established connection was aborted\n",
      "[nltk_data]     by the software in your host machine>\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Artificial', 'JJ'), ('intelligence', 'NN'), ('enables', 'NNS'), ('advanced', 'VBD'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = \"Artificial intelligence enables advanced natural language processing.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
